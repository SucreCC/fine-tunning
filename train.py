"""
è®­ç»ƒå…¥å£è„šæœ¬
"""
import os

# è®¾ç½® tokenizers å¹¶è¡Œæ€§ç¯å¢ƒå˜é‡ï¼Œé¿å… fork è­¦å‘Š
# å¿…é¡»åœ¨å¯¼å…¥ transformers ä¹‹å‰è®¾ç½®
os.environ["TOKENIZERS_PARALLELISM"] = "false"

from transformers import PreTrainedTokenizer
from core.data.custom_dataset import CustomDataset
from core.data.interface.base_processor import BaseProcessor
from core.dto.config.config_manager import ConfigManager
from core.dto.config.log_config import LogConfig
from core.dto.config.model_config import ModelConfig
from core.model.custom_model import CustomModel
from core.training import CustomTrainer
from core.utils import logging
from core.utils.file_utils import find_project_root
from core.utils.logging import setup_logging
from core.utils.seed import set_seed

PROJECT_ROOT = find_project_root()


def init_config():
    """åˆå§‹åŒ–é…ç½®"""
    config_manager = ConfigManager()
    return config_manager.from_yaml()


def init_log(log_config: LogConfig):
    """åˆå§‹åŒ–æ—¥å¿—"""
    setup_logging(log_config)
    return logging.get_logger(__name__)


def init_output_dir(model_config: ModelConfig):
    """åˆå§‹åŒ–è¾“å‡ºç›®å½•"""
    output_dir_path = model_config.get_output_dir()
    output_dir = PROJECT_ROOT / output_dir_path
    output_dir.mkdir(parents=True, exist_ok=True)
    return output_dir


def init_data_set(config_manager: ConfigManager, tokenizer: PreTrainedTokenizer):
    """åˆå§‹åŒ–æ•°æ®é›†"""

    # æ ¹æ®é…ç½®åˆ›å»ºæ•°æ®å¤„ç†å¯¹è±¡
    processor = BaseProcessor.get_processor(config_manager.dataset_config)

    # åˆå§‹åŒ–è®­ç»ƒæ•°æ®é›†
    train_dataset = CustomDataset(
        data_path=config_manager.dataset_config.train_path,
        tokenizer=tokenizer,
        processor=processor,
        max_length=config_manager.dataset_config.max_length,
        train_ratio=config_manager.dataset_config.train_ratio,
        seed=config_manager.training_config.seed
    )

    # åˆå§‹åŒ–éªŒè¯æ•°æ®é›†
    val_dataset = CustomDataset(
        data_path=config_manager.dataset_config.val_path,
        tokenizer=tokenizer,
        processor=processor,
        max_length=config_manager.dataset_config.max_length,
        train_ratio=1.0,  # éªŒè¯é›†ä½¿ç”¨å…¨éƒ¨æ•°æ®
        seed=None  # éªŒè¯é›†ä¸éœ€è¦éšæœºç§å­
    )

    return train_dataset, val_dataset


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    config_manager = init_config()
    logger = init_log(config_manager.log_config)

    # è®¾ç½®éšæœºç§å­
    set_seed(config_manager.training_config.seed)
    logger.info(f"éšæœºç§å­: {config_manager.training_config.seed}")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = init_output_dir(config_manager.model_config)
    logger.info(f"æ¨¡å‹è¾“å‡ºç›®å½•: {output_dir}")

    # åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨
    model, tokenizer = CustomModel(
        model_config=config_manager.model_config,
        finetune_config=config_manager.finetune_config,
    ).load()

    train_dataset, val_dataset = init_data_set(config_manager, tokenizer)
    
    # æ£€æŸ¥æ•°æ®é›†
    if len(train_dataset) == 0:
        logger.error("è®­ç»ƒæ•°æ®é›†ä¸ºç©ºï¼è¯·æ£€æŸ¥æ•°æ®é›†è·¯å¾„å’Œ train_ratio é…ç½®")
        raise ValueError("è®­ç»ƒæ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•å¼€å§‹è®­ç»ƒ")

    # åˆ›å»º Trainerï¼ˆå›è°ƒä¼šè‡ªåŠ¨æ·»åŠ ï¼‰
    logger.info("æ­£åœ¨åˆ›å»º Trainer...")
    trainer = CustomTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        config=config_manager
    )
    logger.info("âœ“ Trainer åˆ›å»ºæˆåŠŸ")
    
    # è¾“å‡ºè®­ç»ƒä¿¡æ¯
    logger.info("=" * 60)
    logger.info("è®­ç»ƒé…ç½®ä¿¡æ¯:")
    logger.info(f"  è®­ç»ƒæ•°æ®é›†å¤§å°: {len(train_dataset)}")
    logger.info(f"  éªŒè¯æ•°æ®é›†å¤§å°: {len(val_dataset) if val_dataset else 0}")
    logger.info(f"  è®­ç»ƒè½®æ•°: {config_manager.training_config.num_epochs}")
    logger.info(f"  æ‰¹æ¬¡å¤§å°: {config_manager.training_config.per_device_train_batch_size}")
    logger.info(f"  æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {config_manager.training_config.gradient_accumulation_steps}")
    effective_batch_size = config_manager.training_config.per_device_train_batch_size * config_manager.training_config.gradient_accumulation_steps
    logger.info(f"  æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {effective_batch_size}")
    logger.info(f"  å­¦ä¹ ç‡: {config_manager.training_config.learning_rate}")
    logger.info(f"  æ—¥å¿—æ­¥æ•°é—´éš”: {config_manager.training_config.logging_steps}")
    logger.info(f"  ä¿å­˜æ­¥æ•°é—´éš”: {config_manager.training_config.save_steps}")
    
    # è®¡ç®—æ€»æ­¥æ•°
    total_steps = (len(train_dataset) // effective_batch_size) * config_manager.training_config.num_epochs
    logger.info(f"  é¢„è®¡æ€»è®­ç»ƒæ­¥æ•°: {total_steps}")
    logger.info("=" * 60)

    # å¼€å§‹è®­ç»ƒ
    logger.info("")
    logger.info("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    logger.info("=" * 60)
    try:
        trainer.train()
        logger.info("=" * 60)
        logger.info("âœ“ è®­ç»ƒå®Œæˆï¼")
    except KeyboardInterrupt:
        logger.info("=" * 60)
        logger.warning("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        raise
    except Exception as e:
        logger.error("=" * 60)
        logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        raise

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    logger.info("ä¿å­˜æ¨¡å‹...")
    model.save_pretrained(str(output_dir))
    tokenizer.save_pretrained(str(output_dir))

    logger.info("è®­ç»ƒå®Œæˆï¼")


if __name__ == "__main__":
    main()
