# ========================
# 服务配置
# ========================
service:
  service_name: mu_xue
  description: mu_xue
  version: 1.0.0

log:
  service_name: mu_xue
  log_dir: ./logs
  log_file: ./logs/mu_xue.log
  error_file: ./logs/mu_xue.error.log
  log_level: INFO
  uvicorn_log_level: INFO
  uvicorn_access_log_level: WARNING
  sqlalchemy_log_level: WARNING
  log_format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  date_format: "%Y-%m-%d %H:%M:%S"

# ========================
# 模型配置
# ========================
model:
  base_model_path: "./model/Qwen2.5-7B-Instruct"
  output_dir: "./outputs/checkpoints"

  # 量化配置（为 QLoRA / LoRA 预留）
  quantization:
    enable: false
    bits: 4                # 4 | 8
    compute_dtype: bf16    # fp16 | bf16

# ========================
# 微调策略配置（核心）
# ========================
finetune:
  type: lora           # full | qlora | prefix_tuning | p_tuning | adalora | ia3
  stage: sft               # sft | dpo | rm（先预留）
  enable: true
  
  # Prefix Tuning 配置（type=prefix_tuning 时生效）
  prefix_tuning:
    num_virtual_tokens: 20
    encoder_hidden_size: null  # null 表示使用模型隐藏层维度
    prefix_projection: false
    task_type: "CAUSAL_LM"
  
  # P-Tuning 配置（type=p_tuning / ptuning 时生效）
  p_tuning:
    num_virtual_tokens: 20
    encoder_hidden_size: null
    encoder_num_layers: 2
    encoder_reparameterization_type: "MLP"  # MLP | LSTM
    task_type: "CAUSAL_LM"
  
  # LoRA 配置（type=lora / qlora 时生效）
  lora:
    r: 8
    lora_alpha: 32
    target_modules: ["q_proj", "v_proj"]
    lora_dropout: 0.1
    init_r: 12
    target_r: 8
    beta1: 0.85
    beta2: 0.85
    task_type: "CAUSAL_LM"
    bias: "none"
  
  # IA3 配置（type=ia3 时生效）
  ia3:
    target_modules: ["k_proj", "v_proj", "out_proj"]
    feedforward_modules: []
    task_type: "CAUSAL_LM"

# ========================
# 数据集配置
# ========================
dataset:
  train_path: "./dataset/moemuu/raw/train.jsonl"
  val_path: "./dataset/moemuu/raw/test.jsonl"
  max_length: 2048
  streaming: false
  train_ratio: 1.0

  # Processor（你设计得很好，保留）
  processor:
    type: qwen
    system_prompt: |
      你是一个名为沐雪的可爱AI女孩子。
      你是一个角色扮演的角色。

# ========================
# 训练参数
# ========================
training:
  num_epochs: 3
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
  learning_rate: 2.0e-5
  weight_decay: 0.01
  lr_scheduler_type: cosine
  warmup_steps: 100
  save_steps: 500
  eval_steps: 500
  logging_steps: 50
  seed: 42
  max_grad_norm: 1.0

  precision:
    fp16: true
    bf16: false

  # 设备配置
  device:
    # 是否使用 CUDA（null 表示自动检测）
    use_cuda: null
    # 分布式训练后端（"nccl" 用于多 GPU，"gloo" 用于 CPU，null 表示不使用分布式）
    # 单机多卡：设置为 "nccl"
    # 多机多卡：设置为 "nccl"，并通过环境变量设置 MASTER_ADDR, MASTER_PORT, WORLD_SIZE, RANK
    ddp_backend: null  # null | nccl | gloo
    # DDP 是否查找未使用的参数（用于调试，通常设为 false）
    ddp_find_unused_parameters: false
    # DDP 超时时间（秒）
    ddp_timeout: 1800
    # 本地 rank（通常从环境变量 LOCAL_RANK 获取，这里可以手动指定）
    local_rank: null


# ========================
# WandB 配置
# ========================
wandb:
  use_wandb: false
  wandb_api_key: null
  wandb_project: "mu_xue_finetuning"
  wandb_run_name: null
  wandb_entity: null
  wandb_tags: null
  wandb_dir: null