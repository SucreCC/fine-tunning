# 模型配置
model:
  # 基础模型路径（可以是 HuggingFace 模型名称或本地路径）
  base_model_path: "THUDM/chatglm3-6b"
  # 输出模型保存路径
  output_dir: "../model"
  # 是否使用 8bit 量化（节省显存）
  use_8bit: false
  # 是否使用 4bit 量化（更节省显存）
  use_4bit: false

# 数据集配置
dataset:
  # 训练集路径
  train_path: "../dataset/moemuu/train.jsonl"
  # 验证集路径
  val_path: "../dataset/moemuu/test.jsonl"
  # 最大序列长度
  max_length: 2048
  # 是否使用流式加载（节省内存）
  streaming: false

# 训练配置
training:
  # 训练轮数
  num_epochs: 3
  # 批次大小
  per_device_train_batch_size: 2
  # 梯度累积步数
  gradient_accumulation_steps: 8
  # 学习率
  learning_rate: 2.0e-5
  # 权重衰减
  weight_decay: 0.01
  # 学习率调度器类型
  lr_scheduler_type: "cosine"
  # 预热步数
  warmup_steps: 100
  # 保存步数间隔
  save_steps: 500
  # 评估步数间隔
  eval_steps: 500
  # 日志记录步数间隔
  logging_steps: 50
  # 是否使用混合精度训练
  fp16: true
  # 是否使用 bf16（需要 A100 等支持）
  bf16: false
  # 随机种子
  seed: 42
  # 最大梯度范数（用于梯度裁剪）
  max_grad_norm: 1.0

# LoRA 配置（如果使用 LoRA 微调）
lora:
  # 是否使用 LoRA
  use_lora: true
  # LoRA rank
  r: 8
  # LoRA alpha
  lora_alpha: 32
  # LoRA target modules（根据模型类型调整）
  target_modules: ["query_key_value", "dense", "dense_h_to_4h", "dense_4h_to_h"]
  # LoRA dropout
  lora_dropout: 0.1

# 其他配置
other:
  # 是否使用 DeepSpeed（需要安装 deepspeed）
  use_deepspeed: false
  # DeepSpeed 配置文件路径
  deepspeed_config: null
  # 是否启用 wandb 日志
  use_wandb: false
  # wandb 项目名称
  wandb_project: "mu_xue_finetuning"

